\section{Esplorazione di NCCL}

\subsection{Introduzione}

\textbf{NVIDIA Collective Communications Library (NCCL)} è una libreria sviluppata da NVIDIA per facilitare operazioni di comunicazione collettiva ad alte prestazioni tra GPU, sia all'interno dello stesso nodo che su nodi distribuiti. NCCL fornisce primitive efficienti per operazioni come \texttt{broadcast}, \texttt{reduce}, \texttt{all-reduce}, \texttt{all-gather} e \texttt{reduce-scatter}, supportando pienamente ambienti multi-GPU e multi-nodo e sfruttando direttamente le interconnessioni ad alta velocità come NVLink, PCIe e InfiniBand.

L'architettura di NCCL è progettata per minimizzare la latenza e massimizzare il throughput delle comunicazioni tra GPU. Le sue caratteristiche principali includono:

\begin{itemize}
    \item \textbf{Topologia consapevole}: NCCL rileva automaticamente la topologia dell’hardware (PCIe, NVLink, InfiniBand) e costruisce percorsi di comunicazione ottimali.
    \item \textbf{Asincronicità}: Le operazioni sono progettate per essere non bloccanti e integrate nel flusso CUDA, permettendo una sovrapposizione efficiente tra comunicazione e computazione.
    \item \textbf{Comunicazione peer-to-peer}: Le GPU si scambiano direttamente i dati, evitando il passaggio attraverso la CPU e migliorando le prestazioni.
\end{itemize}

\subsection{API Principali}
Tra le principali API offerte da NCCL troviamo:
\begin{itemize}
    \item \texttt{ncclCommInitAll} / \texttt{ncclCommInitRank}: inizializzano i comunicatori per tutti i partecipanti al gruppo di comunicazione collettiva.
    \item \texttt{ncclBroadcast}: invia un buffer da una GPU a tutte le altre.
    \item \texttt{ncclAllReduce}: esegue una riduzione (es. somma) dei dati da tutte le GPU e distribuisce il risultato a tutte.
    \item \texttt{ncclReduceScatter}, \texttt{ncclAllGather}, \texttt{ncclReduce}, ecc.: altre primitive collettive standard.
    \item \texttt{ncclGroupStart} / \texttt{ncclGroupEnd}: permettono di raggruppare più operazioni per ottimizzarne l’esecuzione in pipeline.
\end{itemize}

Tutte queste operazioni lavorano direttamente su buffer residenti in memoria device (GPU) e possono essere lanciate su CUDA streams, permettendo un'esecuzione concorrente con il calcolo.

\subsection{Funzionamento}

\subsubsection{Throughput vs MPI}

NCCL è altamente ottimizzata per GPU moderne. In numerosi benchmark, NCCL ha dimostrato throughput superiori rispetto a implementazioni standard di MPI quando si tratta di comunicazioni intra-nodo tra GPU (specialmente via NVLink). Anche nel caso di multi-nodo, NCCL combinata con interconnessioni RDMA (come InfiniBand) può superare le prestazioni di MPI.

\subsubsection{Scalabilità}

La scalabilità è uno dei punti di forza principali:

\begin{itemize}
    \item Ottimizzato per architetture con decine o centinaia di GPU.
    \item Supporto trasparente per configurazioni eterogenee e distribuite.
    \item Adattamento automatico alla topologia hardware.
\end{itemize}

\subsubsection{Vantaggi su MPI}

L'interfaccia di NCCL è molto più semplice rispetto a quella di MPI, con un numero ridotto di funzioni ma fortemente specializzate. Questo rende più facile l’adozione, soprattutto per sviluppatori di applicazioni CUDA, a discapito però di una flessibilità inferiore: ne è un esempio la mancanza di tipi di dato complessi, topologie personalizzate, o comunicazioni puntuali.

\subsection{Svantaggi e Limitazioni}

Nonostante i vantaggi, NCCL presenta alcune limitazioni importanti:

\begin{itemize}
    \item \textbf{Overhead iniziale}: la creazione dei comunicatori nella fase iniziale dell'esecuzione può essere costosa, specialmente in ambienti distribuiti.
    \item \textbf{Meno flessibile di MPI}: NCCL non supporta comunicazioni punto-a-punto né tantomeno operazioni su topologie arbitrarie tra i processi.
    \item \textbf{Limitazioni hardware}: molte ottimizzazioni richiedono il supporto di NVLink o InfiniBand, non sempre presenti nei cluster generici. In assenza di queste il vantaggio in termini di prestazioni può essere ridotto o addirittura nullo.
    \item \textbf{Compatibilità}: la libreria è progettata esclusivamente per GPU NVIDIA e richiede versioni aggiornate del driver e dell’ambiente CUDA.
\end{itemize}

\subsection{Applicazione Potenziale all’Algoritmo SUMMA}

Nel contesto dell’algoritmo SUMMA, NCCL potrebbe essere utilizzata per ottimizzare le fasi di comunicazione collettiva.
L’idea alla base è sostituire le chiamate \texttt{MPI\_Bcast} per la propagazione dei pannelli delle matrici $\mathbf{A}$ e $\mathbf{B}$ lungo le righe e le colonne della griglia di processo, con due chiamate \texttt{ncclBroadcast} operate su due rispettivi comunicatori NCCL. In particolare:

\begin{itemize}
    \item Per ogni passo $k$, il pannello di $\mathbf{A}$ nella riga $i$ viene trasmesso a tutti i processi della riga tramite \texttt{ncclBroadcast}.
    \item Analogamente, il pannello di $\mathbf{B}$ nella colonna $j$ viene trasmesso a tutti i processi della colonna tramite un’altra \texttt{ncclBroadcast}.
\end{itemize}

Per implementare questa strategia sarebbe necessario creare due insiemi di comunicatori NCCL indipendenti, ciascuno rappresentante una delle due dimensioni della griglia logica di processi. L’utilizzo di \texttt{ncclGroupStart} e \texttt{ncclGroupEnd} consentirebbe l’accorpamento delle due comunicazioni in un’unica fase di sincronizzazione, migliorando ulteriormente l'efficienza.
Il codice risultante manterrebbe la stessa logica computazionale di SUMMA, ma con l'intento di sfruttare le ottimizzazioni hardware/software di NCCL nelle fasi di comunicazione collettiva.

Tuttavia, questa implementazione non è stata realizzata a causa della mancanza del supporto a NCCL nel cluster disponibile, rendendo non praticabile la sperimentazione.
