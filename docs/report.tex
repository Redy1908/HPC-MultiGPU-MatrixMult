\documentclass[a4paper]{article}
\usepackage[italian]{babel}
\usepackage[style=ieee,backend=biber]{biblatex} \addbibresource{bibliography.bib}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[table]{xcolor}
\usepackage{minted}

\title{Report PHPC}
\author{Pierluigi Supino \and Rodolfo Diana \and Salvatore Di Gennaro}

\begin{document}

\maketitle
\tableofcontents

\section{Introduzione}

Sviluppare un’applicazione per il prodotto tra matrici su cluster multi-nodo e multi-GPU per nodo (MPI + CUDA, attraverso SLURM). All’interno di questa applicazione devono essere possibili entrambe le seguenti:
\begin{enumerate}
    \item utilizzare kernel originali per i calcoli sulla GPU;
    \item utilizzare funzioni della libreria cuBLAS in alternativa a quelle originali.
\end{enumerate}

\section{Implementazione}
In un ambiente multi-nodo e multi-GPU, il prodotto tra matrici può essere articolato in due livelli distinti ma interconnessi.

A un livello più alto è necessario decomporre il problema a livello globale, ovvero suddividere le matrici da moltiplicare in blocchi che possano essere assegnati in modo efficiente ai diversi nodi del cluster, tenendo conto del bilanciamento del carico, della minimizzazione della comunicazione tra nodi e dell'architettura del sistema.

A un livello più basso, invece, entra in gioco l’effettiva esecuzione del prodotto tra i blocchi locali di matrici all’interno di ciascun nodo, sfruttando le GPU a disposizione. Volendo si potrebbe suddividere questo livello in ulteriori due parti: il partizionamento delle matrice tra le diverse GPU e l'esecuzione dei calcoli reali.

\subsection{Processi}
Il prodotto tra matrici viene eseguito a livello di processi tramite SUMMA (Scalable Universal Matrix Multiplication Algorithm)\cite{SUMMA}.

Suddividiamo i diversi processi una griglia bidimensionale $p \times p$\footnote{In generale l'algoritmo permetterebbe di avere qualsiasi griglia $p \times q$, ma è stato preso in considerazione il caso più semplice.}. L'algoritmo funziona a fasi, in cui per ognuna i processi collaborano per combinare blocchi specifici delle matrici $A$ e $B$. Ogni riga di processi condivide tra sé il blocco di $A$ rilevante per quella fase, mentre ogni colonna fa lo stesso con un blocco di $B$. Questo avviene tramite un’operazione di broadcast.

Una volta ricevuti i blocchi necessari, ogni processo può calcolare una parte del prodotto parziale della matrice $C$, sommando il risultato a quello già accumulato. Ripetendo questa operazione per un numero di fasi pari alla dimensione della griglia (cioè $p^2$ volte), alla fine ogni processo avrà calcolato il suo blocco completo della matrice risultato $C$.

Una volta eseguite tutte le operazioni, resta da raccogliere le diverse sottomatrici e ricomporle. In generale, sono disponibili diversi approcci a seconda di come sia necessario il risultato. In questo caso è stato deciso, per semplicità, che solo il processo 0 debba avere la matrice finale completa.

Nel nostro algoritmo si suppone che le matrici $A$, $B$ e $C$ date in input a ogni processo siano delle dimensioni originali. Per eseguire il broadcast, sono stati creati dei tipi derivati appositi per tenere in considerazione gli spazi di memoria che intercorrono tra una riga e l'altra dei blocchi da inviare.
\begin{table}[h]
    \center
    \begin{tabular}{|l|l|l|l|}
        \hline
        \cellcolor{yellow}0 & \cellcolor{yellow}1 & 2  & 3  \\ \hline
        \cellcolor{yellow}4 & \cellcolor{yellow}5 & 6  & 7  \\ \hline
        8                   & 9                   & 10 & 11 \\ \hline
        12                  & 13                  & 14 & 15 \\ \hline
    \end{tabular}
    \begin{tabular}{|l|l|l|l|l|l|l|l|l}
        \hline
        \cellcolor{yellow}0 & \cellcolor{yellow}1 & 2 & 3 & \cellcolor{yellow}4 & \cellcolor{yellow}5 & 6 & 7 & \dots \\ \hline
    \end{tabular}
\end{table}

\subsection{GPU}
\subsubsection{Multi-GPU}
Per sfruttare al massimo le risorse disponibili, è stato necessario sviluppare un sistema che permettesse la suddivisione del lavoro su più GPU presenti nella macchina.

L'obiettivo consiste nel sovrapporre quanto più possibile il lavoro svolto dalle diverse GPU ed evitare conflitti di memoria che dovrebbero essere serializzati, in particolare la scrittura sulla matrice $C$. Un'idea naturale è partizionare quest'ultima in modo che ogni device possa eseguire le operazioni in uno spazio separato dagli altri.
Una suddivisione semplice ma funzionale per qualsiasi numero di GPU è quella per colonne: ogni GPU effettua quindi il prodotto tra la matrice A e la ``colonna'' assegnata della matrice B.

% TODO: inserire schema

% \begin{table}[h]
%     \begin{tabular}{|c|c|}
%         \hline
%               &       \\
%         $B_0$ & $B_1$ \\
%               &       \\ \hline
%     \end{tabular}

%     \begin{tabular}{|c|c|}
%         \hline
%          & \\
%          & \\
%          & \\ \hline
%     \end{tabular}
%     \begin{tabular}{|c|c|c|}
%         \hline
%               &       &       \\
%         $C_0$ & \dots & $C_1$ \\
%               &       &       \\ \hline
%     \end{tabular}
% \end{table}

Questo meccanismo implica, ovviamente, che l'host debba essere in grado di avviare l'esecuzione su più GPU senza dover attendere prima il completamento di una di esse.
Un metodo sicuramente valido è creare diversi thread a livello di processo, ognuno dei quali gestisca un determinato device.
Tuttavia, CUDA offre anche varianti asincrone delle diverse funzioni e la possibilità di gestirle tramite \textit{stream}, ovvero code di operazioni da eseguire in sequenza.

% TODO: aggiungere pseudocodice

\subsubsection{Kernel}
Per quanto riguarda l'implementazione della funzione kernel che esegue il prodotto sulla GPU, l'idea è che ogni thread avviato calcoli il prodotto scalare delle righe di $A$ e delle colonne di $B$ corrispondenti.

% utilizzo della shared memory
La GPU presenta, oltre alla memoria globale, anche una regione di memoria di dimensioni molto minori ma collocata direttamente sul chip e per questo molto più veloce. Possiamo sfruttarla per caricare un blocco di $A$ e di $B$ dalla memoria globale ed eseguire quanto più lavoro possibile su di essi.

% Tile quantization
Bisogna gestire il caso in cui i thread avviati siano minori delle dimensioni delle matrici, quindi ogni thread potrebbe dover calcolare più celle.
Pur assicurandoci che non siano effettuati accessi di memoria non validi, tutti i blocchi eseguiranno lo stesso numero di operazioni. Il numero di operazioni eseguite realmente potrebbe essere molte più grande di quante ne siano effettivamente necessarie. Pertanto l'efficienza massima si ha quando le dimensioni della matrice sono divisibili per le dimensioni della griglia.

\subsubsection{cuBLAS}

\section{Analisi}

\subsection{Configurazione dei test}

\subsection{Analisi delle misurazioni}

\subsection{NVIDIA Nsight Compute}

\section{Esplorazione di NNCL}

\subsection{Introduzione}

\subsection{Possibili implementazioni}

\printbibliography

\end{document}